{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\anaconda3\\lib\\site-packages (4.0.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\anaconda3\\lib\\site-packages (from gensim) (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\anaconda3\\lib\\site-packages (from gensim) (1.19.5)\n",
      "Requirement already satisfied: Cython==0.29.21 in c:\\anaconda3\\lib\\site-packages (from gensim) (0.29.21)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\anaconda3\\lib\\site-packages (from gensim) (5.0.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n",
      "[==================================================] 100.0% 104.8/104.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader\n",
    "# download the word2vec model with common_texts\n",
    "\n",
    "model = Word2Vec(sentences=common_texts, vector_size=100, window=5, min_count=1, workers=4)\n",
    "# sims = model.wv.most_similar('computer', topn=10)  # get other similar words\n",
    "print(list(gensim.downloader.info()['models'].keys()))\n",
    "glove_vectors = gensim.downloader.load('glove-twitter-25')      # It's slow\n",
    "wv = glove_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1193514\n"
     ]
    }
   ],
   "source": [
    "print(len(wv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of traininig data: 4641\n",
      "length of devolop data:  580\n",
      "length of test data:  581\n"
     ]
    }
   ],
   "source": [
    "# Load training data as train_data， write into train_file, test_file and label_file\n",
    "\n",
    "import jsonlines\n",
    "\n",
    "path = 'project-data/'\n",
    "train_data_path = 'train.data.jsonl'\n",
    "dev_data_path = 'dev.data.jsonl'\n",
    "test_data_path = 'test.data.jsonl'\n",
    "\n",
    "train_data = []\n",
    "with jsonlines.open(path + train_data_path) as reader:\n",
    "    for obj in reader:\n",
    "        train_data.append(obj)\n",
    "print('length of traininig data:', len(train_data))\n",
    "\n",
    "        \n",
    "# load the development data as dev_data(used as test_data in RvNN)\n",
    "dev_data = []\n",
    "with jsonlines.open(path + dev_data_path) as reader1:\n",
    "    for obj in reader1:\n",
    "        dev_data.append(obj)\n",
    "print('length of devolop data: ', len(dev_data))\n",
    "\n",
    "\n",
    "test_data = []\n",
    "with jsonlines.open(path + test_data_path) as reader2:\n",
    "    for obj in reader2:\n",
    "        test_data.append(obj)\n",
    "print('length of test data: ', len(test_data))\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "train_label_path = 'train.label.json'\n",
    "dev_label_path = 'dev.label.json'\n",
    "with open(path+train_label_path) as f1, open(path+dev_label_path) as f2:\n",
    "    train_label = json.load(f1)\n",
    "    dev_label = json.load(f2)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the three dataset in time_stamp order\n",
    "\n",
    "# Wed Jan 07 14:04:29 +0000 2015\n",
    "def time_stamp(dic):\n",
    "    # given a tweet dict\n",
    "    month_list = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', \"Aug\", 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "    time = dic['created_at']\n",
    "    month = time[4:7]\n",
    "    day = time[8:10]\n",
    "    h = time[11:13]\n",
    "    m = time[14:16]\n",
    "    s = time[17:19]\n",
    "    return month_list.index(month)*86400*30 + int(day)*86400 + int(h)*3600 + int(m)*60 + int(s)*1\n",
    "\n",
    "\n",
    "for idx in range(len(train_data)):\n",
    "    train_data[idx] = sorted(train_data[idx], key=lambda tree_node: time_stamp(tree_node))\n",
    "\n",
    "for idx in range(len(test_data)):\n",
    "    test_data[idx] = sorted(test_data[idx], key=lambda tree_node: time_stamp(tree_node))\n",
    "\n",
    "for idx in range(len(dev_data)):\n",
    "    dev_data[idx] = sorted(dev_data[idx], key=lambda tree_node: time_stamp(tree_node))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl = []\n",
    "for idx in range(len(train_data)):\n",
    "    Id = train_data[idx][0]['id_str']\n",
    "    l = train_label[Id]\n",
    "    if l == 'rumour':\n",
    "        tl.append([1.0, 0.0])\n",
    "    if l == 'non-rumour':\n",
    "        tl.append([0.0, 1.0])\n",
    "    \n",
    "dl = []\n",
    "for idx in range(len(dev_data)):\n",
    "    Id = dev_data[idx][0]['id_str']\n",
    "    l = dev_label[Id]\n",
    "    if l == 'rumour':\n",
    "        dl.append(1)\n",
    "    if l == 'non-rumour':\n",
    "        dl.append(0)\n",
    "    \n",
    "    \n",
    "save_obj(tl, 'train_label')\n",
    "save_obj(dl, 'dev_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get All texts and process them into Text and Dev\n",
    "\n",
    "\n",
    "Train_txt = []\n",
    "for tree in train_data:\n",
    "    sub_text = []\n",
    "    for elem in tree:\n",
    "        sub_text.append(elem['text'])\n",
    "    Train_txt.append(sub_text)\n",
    "    \n",
    "Dev_txt = []\n",
    "for tree in dev_data:\n",
    "    sub_text = []\n",
    "    for elem in tree:\n",
    "        sub_text.append(elem['text'])\n",
    "    Dev_txt.append(sub_text)\n",
    "    \n",
    "Test_txt = []\n",
    "for tree in test_data:\n",
    "    sub_text = []\n",
    "    for elem in tree:\n",
    "        sub_text.append(elem['text'])\n",
    "    Test_txt.append(sub_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aubd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\aubd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corpus size:  42891\n",
      "respond\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import copy, re    # We need regular expression to filter tokens.\n",
    "tt = TweetTokenizer()\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "stopwords = set(stopwords.words('english')) #note: stopwords are all in lowercase\n",
    "# import these modules\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "Text_list = [Train_txt, Dev_txt, Test_txt]\n",
    "\n",
    "\n",
    "def preprocess_text(data, dev, test):\n",
    "    '''\n",
    "    1. Tokenize the tweets\n",
    "    2. lowercase and remove non-English words and stopwords\n",
    "    3. Lemma\n",
    "    '''\n",
    "    word_dict = {} # Word dictionary\n",
    "    res = []       # All processed texts\n",
    "    sentences = []\n",
    "    \n",
    "    x = copy.deepcopy(data)\n",
    "    processed_train = []\n",
    "    for tree in x:\n",
    "        sub_res = []\n",
    "        for elem in tree:\n",
    "            # 1. Tokenize the Tweets\n",
    "            temp = tt.tokenize(elem)\n",
    "            # 2. Trans all word into lowercase\n",
    "            temp = [token.lower() for token in temp]\n",
    "            # 3. Remove non-English words and remove stopwords\n",
    "            temp = [token for token in temp if re.match(r'.*[a-z]+.*', token) \n",
    "                and token not in stopwords]\n",
    "            \n",
    "            # 4. Word Lemmatize\n",
    "            # HTTP 和 @ 视作单独一类\n",
    "            for idx in range(len(temp)):\n",
    "                if temp[idx][:4] == 'http':\n",
    "                    temp[idx] = 'http'\n",
    "                if temp[idx][:1] == '@':\n",
    "                    temp[idx] = '@'\n",
    "                if temp[idx][:1] == '#':\n",
    "                    temp[idx] = temp[idx][1:]\n",
    "                temp[idx] = lemma.lemmatize(temp[idx])\n",
    "                \n",
    "                \n",
    "                if temp[idx] not in word_dict:\n",
    "                    word_dict[temp[idx]] = 1\n",
    "                else:\n",
    "                    word_dict[temp[idx]] += 1\n",
    "            \n",
    "            sentences.append(temp)\n",
    "            sub_res.append(temp)\n",
    "        processed_train.append(sub_res)\n",
    "        \n",
    "        \n",
    "    x1 = copy.deepcopy(dev)\n",
    "    processed_dev = []\n",
    "    for tree in x1:\n",
    "        sub_res = []\n",
    "        for elem in tree:\n",
    "            temp = tt.tokenize(elem)\n",
    "            temp = [token.lower() for token in temp]\n",
    "            temp = [token for token in temp if re.match(r'.*[a-z]+.*', token) \n",
    "                and token not in stopwords]\n",
    "            for idx in range(len(temp)):\n",
    "                if temp[idx][:4] == 'http':\n",
    "                    temp[idx] = 'http'\n",
    "                if temp[idx][:1] == '@':\n",
    "                    temp[idx] = '@'\n",
    "                if temp[idx][:1] == '#':\n",
    "                    temp[idx] = temp[idx][1:]\n",
    "                temp[idx] = lemma.lemmatize(temp[idx])\n",
    "                if temp[idx] not in word_dict:\n",
    "                    word_dict[temp[idx]] = 1\n",
    "                else:\n",
    "                    word_dict[temp[idx]] += 1\n",
    "            sentences.append(temp)\n",
    "            sub_res.append(temp)\n",
    "        processed_dev.append(sub_res)\n",
    "        \n",
    "    x2 = copy.deepcopy(test)\n",
    "    processed_test = []\n",
    "    for tree in x2:\n",
    "        sub_res = []\n",
    "        for elem in tree:\n",
    "            temp = tt.tokenize(elem)\n",
    "            temp = [token.lower() for token in temp]\n",
    "            temp = [token for token in temp if re.match(r'.*[a-z]+.*', token) \n",
    "                and token not in stopwords]\n",
    "            for idx in range(len(temp)):\n",
    "                if temp[idx][:4] == 'http':\n",
    "                    temp[idx] = 'http'\n",
    "                if temp[idx][:1] == '@':\n",
    "                    temp[idx] = '@'\n",
    "                if temp[idx][:1] == '#':\n",
    "                    temp[idx] = temp[idx][1:]\n",
    "                temp[idx] = lemma.lemmatize(temp[idx])\n",
    "                if temp[idx] not in word_dict:\n",
    "                    word_dict[temp[idx]] = 1\n",
    "                else:\n",
    "                    word_dict[temp[idx]] += 1\n",
    "            sentences.append(temp)                \n",
    "            sub_res.append(temp)\n",
    "        processed_test.append(sub_res)\n",
    "      \n",
    "    res = [processed_train, processed_dev, processed_test]\n",
    "    return res, word_dict, sentences\n",
    "\n",
    "new_Text, word_dict, sentences = preprocess_text(Train_txt, Dev_txt, Test_txt)\n",
    "print('The corpus size: ', len(word_dict))\n",
    "print(new_Text[0][0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=sentences, vector_size=25, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42891\n",
      "42891\n",
      "[-0.02967359  0.01701257  0.00399996 -0.04300635 -0.02336247  0.02883195\n",
      "  0.07243662  0.0523413  -0.06890876 -0.06089007 -0.03625345 -0.03836469\n",
      "  0.01579831  0.04279788 -0.03704663  0.05228771 -0.01651263 -0.02161884\n",
      " -0.01301975  0.03123043  0.04063087  0.0308115   0.04952277  0.01623834\n",
      "  0.06045783]\n"
     ]
    }
   ],
   "source": [
    "print(len(model.wv))\n",
    "print(len(word_dict.keys()))\n",
    "print(model.wv['c4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "respond\n"
     ]
    }
   ],
   "source": [
    "train_set, dev_set, test_set = new_Text[0], new_Text[1], new_Text[2]\n",
    "print(train_set[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(train_set)):\n",
    "    # tree: train_set[idx]\n",
    "    for sen_idx in range(len(train_set[idx])):\n",
    "        # sentence: train_set[idx][sen_idx]\n",
    "        for word_idx in range(len(train_set[idx][sen_idx])):\n",
    "            # word: train_set[idx][sen_idx][word_idx]\n",
    "            #print(train_set[idx][sen_idx][word_idx])\n",
    "            word_vec = model.wv[train_set[idx][sen_idx][word_idx]]\n",
    "            train_set[idx][sen_idx][word_idx] = word_vec\n",
    "        \n",
    "for idx in range(len(dev_set)):\n",
    "    # tree: dev_set[idx]\n",
    "    for sen_idx in range(len(dev_set[idx])):\n",
    "        # sentence: dev_set[idx][sen_idx]\n",
    "        for word_idx in range(len(dev_set[idx][sen_idx])):\n",
    "            # word: dev_set[idx][sen_idx][word_idx]\n",
    "            #print(dev_set[idx][sen_idx][word_idx])\n",
    "            word_vec = model.wv[dev_set[idx][sen_idx][word_idx]]\n",
    "            dev_set[idx][sen_idx][word_idx] = word_vec\n",
    "\n",
    "for idx in range(len(test_set)):\n",
    "    # tree: test_set[idx]\n",
    "    for sen_idx in range(len(test_set[idx])):\n",
    "        # sentence: test_set[idx][sen_idx]\n",
    "        for word_idx in range(len(test_set[idx][sen_idx])):\n",
    "            # word: test_set[idx][sen_idx][word_idx]\n",
    "            #print(test_set[idx][sen_idx][word_idx])\n",
    "            word_vec = model.wv[test_set[idx][sen_idx][word_idx]]\n",
    "            test_set[idx][sen_idx][word_idx] = word_vec\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "dirname = 'data/'\n",
    "def save_obj(obj, name ):\n",
    "    with open(dirname+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open( dirname + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "save_obj(train_set, 'train_set')\n",
    "save_obj(dev_set,   'dev_set')\n",
    "save_obj(test_set,  'test_set')\n",
    "save_obj(train_label, 'train_label')\n",
    "save_obj(dev_label, 'dev_label')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
